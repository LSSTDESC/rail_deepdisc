{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c2a2dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training script for LazyConfig models\n",
    "try:\n",
    "    # ignore ShapelyDeprecationWarning from fvcore\n",
    "    import warnings\n",
    "\n",
    "    from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning)\n",
    "\n",
    "except:\n",
    "    pass\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "\n",
    "import detectron2.utils.comm as comm\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2.config import LazyConfig, get_cfg\n",
    "import detectron2.data as data\n",
    "from detectron2.engine import (\n",
    "    launch,\n",
    ")\n",
    "\n",
    "from deepdisc.data_format.augment_image import train_augs\n",
    "from deepdisc.data_format.image_readers import DC2ImageReader\n",
    "from deepdisc.data_format.register_data import (\n",
    "    register_data_set,\n",
    ")  # , register_loaded_data_set\n",
    "from deepdisc.model.loaders import (\n",
    "    RedshiftFlatDictMapper,\n",
    "    return_test_loader,\n",
    "    return_train_loader,\n",
    ")\n",
    "from deepdisc.model.models import (\n",
    "    RedshiftPointCasROIHeads,\n",
    "    RedshiftPointROIHeads,\n",
    "    RedshiftPDFROIHeads,\n",
    "    return_lazy_model,\n",
    ")\n",
    "from deepdisc.training.trainers import (\n",
    "    return_evallosshook,\n",
    "    return_lazy_trainer,\n",
    "    return_optimizer,\n",
    "    return_savehook,\n",
    "    return_schedulerhook,\n",
    ")\n",
    "from deepdisc.utils.parse_arguments import make_training_arg_parser\n",
    "from deepdisc.inference.predictors import return_predictor_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5244b7ff-1501-47ad-b74e-c575f9758089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rail.estimation.algos.deepdisc import DeepDiscInformer\n",
    "import rail\n",
    "from rail.estimation.algos.deepdisc import *\n",
    "from rail.core.data import TableHandle, JsonHandle\n",
    "from rail.core.stage import RailStage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e82e6de-f6ec-46b5-a6a7-60e79cd79c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfgfile = \"./tests/configs/solo/solo_res50_DC2.py\"\n",
    "cfgfile = \"./configs/solo/solo_swin_DC2_weighted.py\"\n",
    "\n",
    "output_dir = \"./\"\n",
    "output_name = \"test\"\n",
    "\n",
    "trainfile = \"/home/shared/hsc/DC2/test_data/dataset_3/flattened_images_train.hdf5\"\n",
    "testfile = \"/home/shared/hsc/DC2/test_data/dataset_3/flattened_images_test.hdf5\"\n",
    "metadatafile = \"/home/shared/hsc/DC2/test_data/dataset_3/train_metadata.hdf5\"\n",
    "test_metadatafile = \"/home/shared/hsc/DC2/test_data/dataset_3/test_metadata.hdf5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd06982-1e98-4b8a-a3a8-b68ac71cd6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01f48f9e-16a3-423d-a550-ed9298b6fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = DS.add_data(\n",
    "    \"training\", data=None, handle_class=TableHandle, path=trainfile\n",
    ")  # ()[\"images\"]\n",
    "testing = DS.add_data(\"testing\", data=None, handle_class=TableHandle, path=testfile)\n",
    "\n",
    "#metadatahandle = DS.add_data(\"metadata\", metadata, JsonHandle, path=metadatafile)\n",
    "metadatahandle = DS.add_data(\"metadata\", data=None, handle_class=Hdf5Handle, path=metadatafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a57e206a-c5c6-4526-a278-868ef2a6047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_dict = dict(\n",
    "    chunk_size=5,\n",
    "    epoch=20,\n",
    "    batch_size=1,\n",
    "    output_dir=\"./\",\n",
    "    cfgfile = cfgfile,\n",
    "    num_gpus=1,\n",
    "    print_frequency=10,\n",
    "    head_epochs=3,\n",
    "    full_epochs=3,\n",
    "    mile1=0,\n",
    "    mile2=0,\n",
    "    training_percent=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d611ae20-a7c7-4f52-9390-1d27eca138f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Inform = DeepDiscInformer.make_stage(\n",
    "    name=\"Inform_DeepDISC\", model=\"test_informer.pkl\", **deep_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b6a9d1-697e-4bde-abee-e519253daa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching data\n",
      "Training head layers\n",
      "\u001b[32m[01/11 15:51:58 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[01/11 15:51:58 d2.data.common]: \u001b[0mSerializing 800 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[01/11 15:52:02 d2.data.common]: \u001b[0mSerialized dataset takes 223.74 MiB\n",
      "\u001b[32m[01/11 15:52:02 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=1\n",
      "\u001b[32m[01/11 15:52:02 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[01/11 15:52:02 d2.data.common]: \u001b[0mSerializing 200 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[01/11 15:52:09 d2.data.common]: \u001b[0mSerialized dataset takes 55.56 MiB\n",
      "\u001b[32m[01/11 15:52:32 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /home/shared/hsc/detectron2/projects/ViTDet/model_final_246a82.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'backbone.bottom_up.patch_embed.proj.weight' to the model due to incompatible shapes: (128, 3, 4, 4) in the checkpoint but (128, 6, 4, 4) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.0.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.0.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.1.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.1.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.2.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.2.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.bottom_up.patch_embed.proj.weight\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.0.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.1.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.2.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.dummy_param\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.redshift_fc.0.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.redshift_fc.2.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.redshift_fc.4.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/11 15:52:33 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "<detectron2.solver.lr_scheduler.LRMultiplier object at 0x7ffd0c349b80>\n",
      "Iteration:  10  data time:  0.263813559897244  loss time:  0.22456208523362875 dict_keys(['loss_cls_stage0', 'loss_box_reg_stage0', 'loss_cls_stage1', 'loss_box_reg_stage1', 'loss_cls_stage2', 'loss_box_reg_stage2', 'loss_mask', 'redshift_loss', 'loss_rpn_cls', 'loss_rpn_loc']) [0.6415848135948181, 0.23508834838867188, 0.75870281457901, 0.48844820261001587, 0.6668554544448853, 1.1540870666503906, 0.5557609796524048, 0.6763800462640401, 0.5458568930625916, 0.22760748863220215] val loss:  0 lr:  [0.001]\n",
      "Iteration:  20  data time:  0.36718152835965157  loss time:  0.2874174118041992 dict_keys(['loss_cls_stage0', 'loss_box_reg_stage0', 'loss_cls_stage1', 'loss_box_reg_stage1', 'loss_cls_stage2', 'loss_box_reg_stage2', 'loss_mask', 'redshift_loss', 'loss_rpn_cls', 'loss_rpn_loc']) [0.5144137740135193, 0.2540614902973175, 0.4824583828449249, 0.6232739686965942, 0.43338853120803833, 1.468063473701477, 0.41160714626312256, 0.5072423474000403, 0.22151610255241394, 0.171247661113739] val loss:  0 lr:  [0.001]\n"
     ]
    }
   ],
   "source": [
    "# Inform.inform(training, metadatahandle)\n",
    "Inform.inform(training, metadatahandle) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c69b958",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7070dcb2-0de8-42af-af8d-f40c689b7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadatahandle = DS.add_data(\n",
    "#     \"metadata\", test_metadata, JsonHandle, path=test_metadatafile\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7360caf2-5b3d-4998-be0c-3ca28fb5e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator = DeepDiscPDFEstimator.make_stage(\n",
    "#     name=\"DeepDiscEstimator\",\n",
    "#     model=Inform.get_handle(\"model\"),\n",
    "#     # hdf5_groupname=\"images\",\n",
    "#     **deep_dict,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415a174-6d14-48f5-9241-963a84431501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estimator.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74bb222-f53c-415c-acab-f8b978a2af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = Estimator.estimate(testing, metadatahandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b402ba2-3779-44e7-8654-113f19731464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = results.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2b4a5-0422-4f8f-8e08-49811549c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truth = Estimator.get_handle(\"truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da4c4a-93c6-4203-a1b3-4fe186bad242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ztrue = truth.data[\"redshift\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48290de-6f18-422c-9b04-c6aaec97439f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c712fc8-1e60-4389-86fb-2864922fe19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a243e22-46b1-47a2-9c31-9e0746dbee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from rail.core.data import ModelHandle\n",
    "MH = ModelHandle('model',path='./test_informer.pkl')\n",
    "data = MH.read()\n",
    "#cfgfile = \"./configs/solo/solo_swin_DC2.py\"\n",
    "#cfg = LazyConfig.load(cfgfile)\n",
    "#checkpoint = MH()['nnmodel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15fb9820-e604-4fff-95ba-c549311ebd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d93d6cc-333c-4c28-b5df-f295433cc9e7",
   "metadata": {},
   "source": [
    "## Test new chunking algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48936320-ed0f-40c3-8329-1f3e2c7c31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_estimation_dict = dict(\n",
    "    chunk_size=100,\n",
    "    output_dir=\"./\",\n",
    "    cfgfile = cfgfile,\n",
    "    zmin=0,\n",
    "    zmax=5,\n",
    "    nzbins=200,\n",
    "    output_mode='default',\n",
    "    include_ids=True\n",
    ")\n",
    "\n",
    "EstimatorWithChunks = DeepDiscPDFEstimatorWithChunking.make_stage(\n",
    "    name=\"DeepDiscEstimatorWithChunks\",\n",
    "    #model=Inform.get_handle(\"model\"),\n",
    "    model='/home/g4merz/DC2/model_tests/Swin_NG3_weighted_informer.pkl',\n",
    "    **deep_estimation_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31f8a0c3-6ad4-430c-9892-b70ae4285ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_file_for_chunks = \"/home/shared/hsc/DC2/test_data/dataset_3/flattened_images_test_small.hdf5\"\n",
    "test_file_for_chunks = \"/home/shared/hsc/DC2/test_data/dataset_3/flattened_images_test.hdf5\"\n",
    "\n",
    "test_handle_for_chunks = DS.add_data(\"testing\", data=None, handle_class=TableHandle, path=test_file_for_chunks)\n",
    "#metadatafile_with_chunks = \"/home/shared/hsc/DC2/test_data/dataset_3/test_metadata_example.hdf5\"\n",
    "metadatafile_with_chunks = \"/home/shared/hsc/DC2/test_data/dataset_3/test_metadata.hdf5\"\n",
    "\n",
    "metadatahandle_with_chunks = DS.add_data(\"metadata\", data=None, handle_class=Hdf5Handle, path=metadatafile_with_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c684e258-6c71-476f-bf35-79b53df86282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting handle into data store.  model: ./test_informer.pkl, DeepDiscEstimatorWithChunks\n",
      "Caching data\n",
      "Processing chunk (start:end) - (0:5)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (5:10)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (10:15)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (15:20)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (20:25)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (25:30)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (30:35)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (35:40)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (40:45)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (45:50)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (50:55)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (55:60)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (60:65)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (65:70)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (70:75)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (75:80)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (80:85)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (85:90)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (90:95)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (95:100)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Inserting handle into data store.  output_DeepDiscEstimatorWithChunks: inprogress_output_DeepDiscEstimatorWithChunks.hdf5, DeepDiscEstimatorWithChunks\n"
     ]
    }
   ],
   "source": [
    "results_from_chunks = EstimatorWithChunks.estimate(test_handle_for_chunks, metadatahandle_with_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e8f2a02-2aec-46cb-b963-c32b3caa0711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ens = results_from_chunks.read()\n",
    "res_ens.npdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57dee5cd-d2ad-4132-83a9-3845aeb8bbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': array([[0.52763819],\n",
       "        [0.57788945],\n",
       "        [0.55276382],\n",
       "        [0.60301508],\n",
       "        [0.65326633],\n",
       "        [0.65326633],\n",
       "        [0.50251256],\n",
       "        [0.6281407 ],\n",
       "        [0.6281407 ],\n",
       "        [0.47738693],\n",
       "        [0.65326633],\n",
       "        [0.65326633],\n",
       "        [0.55276382],\n",
       "        [0.55276382],\n",
       "        [0.60301508],\n",
       "        [0.70351759],\n",
       "        [0.85427136],\n",
       "        [0.65326633],\n",
       "        [0.6281407 ],\n",
       "        [0.65326633],\n",
       "        [0.55276382],\n",
       "        [0.52763819],\n",
       "        [0.60301508],\n",
       "        [0.70351759],\n",
       "        [0.75376884],\n",
       "        [0.55276382],\n",
       "        [0.52763819],\n",
       "        [0.60301508],\n",
       "        [0.52763819],\n",
       "        [0.57788945],\n",
       "        [0.6281407 ],\n",
       "        [0.6281407 ],\n",
       "        [0.60301508],\n",
       "        [0.47738693],\n",
       "        [0.65326633],\n",
       "        [0.67839196],\n",
       "        [0.67839196],\n",
       "        [0.65326633],\n",
       "        [0.50251256],\n",
       "        [0.60301508],\n",
       "        [0.60301508],\n",
       "        [0.57788945],\n",
       "        [0.57788945],\n",
       "        [0.52763819],\n",
       "        [0.60301508],\n",
       "        [0.6281407 ],\n",
       "        [0.60301508],\n",
       "        [0.67839196],\n",
       "        [0.52763819],\n",
       "        [0.65326633],\n",
       "        [0.57788945],\n",
       "        [0.60301508],\n",
       "        [0.55276382],\n",
       "        [0.6281407 ],\n",
       "        [0.47738693],\n",
       "        [0.70351759],\n",
       "        [0.57788945],\n",
       "        [0.52763819],\n",
       "        [0.52763819],\n",
       "        [0.50251256],\n",
       "        [0.60301508],\n",
       "        [0.52763819],\n",
       "        [0.60301508],\n",
       "        [0.77889447],\n",
       "        [0.6281407 ],\n",
       "        [0.57788945],\n",
       "        [0.6281407 ],\n",
       "        [0.65326633],\n",
       "        [0.6281407 ],\n",
       "        [0.67839196],\n",
       "        [0.70351759],\n",
       "        [0.52763819],\n",
       "        [0.65326633],\n",
       "        [0.55276382],\n",
       "        [0.60301508],\n",
       "        [0.60301508],\n",
       "        [0.55276382],\n",
       "        [0.52763819],\n",
       "        [0.57788945],\n",
       "        [0.57788945],\n",
       "        [0.60301508],\n",
       "        [0.6281407 ],\n",
       "        [0.55276382],\n",
       "        [0.67839196],\n",
       "        [0.60301508],\n",
       "        [0.52763819],\n",
       "        [0.55276382]]),\n",
       " 'true_zs': array([0.7012318 , 0.42778328, 1.7992706 , 1.1620783 , 0.9963913 ,\n",
       "        1.2140867 , 0.5360316 , 1.0547436 , 1.4523635 , 0.7515907 ,\n",
       "        1.0547494 , 0.68010664, 0.5363086 , 0.6793133 , 0.5734861 ,\n",
       "        0.42558432, 0.60308284, 0.3529509 , 0.24118459, 1.2155589 ,\n",
       "        0.9058581 , 0.8509604 , 0.95761937, 0.70490974, 1.1069846 ,\n",
       "        0.85279965, 0.95642143, 0.7487691 , 1.1063808 , 1.2125955 ,\n",
       "        1.6524489 , 0.3594582 , 0.818352  , 0.3305829 , 0.83958596,\n",
       "        1.1430179 , 1.1344079 , 0.90554786, 0.29094383, 0.10626756,\n",
       "        0.8646555 , 0.5388248 , 0.8387574 , 0.        , 1.5598063 ,\n",
       "        1.1590296 , 0.15972891, 1.4697758 , 0.7191161 , 0.82555753,\n",
       "        0.7123359 , 0.6165088 , 0.7302246 , 0.29236817, 0.3486897 ,\n",
       "        0.8896456 , 1.6743237 , 0.42754424, 1.4256082 , 0.        ,\n",
       "        0.75538224, 0.55634505, 0.18478893, 0.7140138 , 1.0169499 ,\n",
       "        1.105972  , 1.48604   , 1.4866827 , 0.75717014, 0.38609636,\n",
       "        0.6558393 , 0.        , 1.4186352 , 0.96277624, 1.1484878 ,\n",
       "        0.18595909, 0.6352321 , 0.83982754, 1.3856164 , 1.1214628 ,\n",
       "        0.58828187, 1.3282484 , 0.48820096, 1.255218  , 0.3061983 ,\n",
       "        1.2140708 , 0.3714101 ]),\n",
       " 'ids': array([16836280390288443, 16836280390288086, 16836280390289617,\n",
       "        16836280390258727, 16836280390289083, 16836280390290112,\n",
       "        16836280390290289, 16836280390289622, 16836280390289322,\n",
       "        16836280390288431, 16836280390289618, 16836280390289860,\n",
       "        16836280390289885, 16836280390287967, 16836280390289023,\n",
       "        16836280390288264, 16836280390290296, 16836280390258539,\n",
       "        16836280390288349, 16836280390289619, 16836430714144494,\n",
       "        16836430714145300, 16836430714113689, 16836430714145054,\n",
       "        16836430714144041, 16836430714145301, 16836430714144824,\n",
       "        16836430714113544, 16836430714143939, 16836430714143437,\n",
       "        16836430714144355, 16836430714145346, 16836430714145088,\n",
       "        16836430714145386, 16836430714143909, 16836430714145053,\n",
       "        16836430714143438, 16836430714144265, 16836293275167060,\n",
       "        16836297570152349, 16836293275200796, 16836293275200429,\n",
       "        16836297570152437, 16836293275201067, 16836293275200119,\n",
       "        16836293275199779, 16836293275199842, 16836293275200123,\n",
       "        16836293275200067, 16836293275200963, 16836293275200926,\n",
       "        16836293275200115, 16840674141840530, 16840674141842303,\n",
       "        16840674141840974, 16840674141842304, 16840674141842033,\n",
       "        16840674141842041, 16840674141840521, 16840674141842146,\n",
       "        16840674141841756, 16840674141842043, 16840674141841652,\n",
       "        16840674141840173, 16840674141840781, 16840674141840975,\n",
       "        16840674141842040, 16840674141842149, 16840674141840799,\n",
       "        16840674141842125, 16840674141842147, 16840828760653648,\n",
       "        16840828760624437, 16840828760654092, 16840828760623972,\n",
       "        16840828760623996, 16840828760652442, 16840828760624926,\n",
       "        16840828760653818, 16840828760653452, 16840828760652199,\n",
       "        16840828760653453, 16840828760652685, 16840828760653671,\n",
       "        16840828760624408, 16840828760653878, 16840828760653396])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ens.ancil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac614133-deb4-4fd6-be40-ed5750ff13e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = EstimatorWithChunks.get_handle(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1513a0-2297-4c6c-807a-83faa995bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.data.ancil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071207a1-b3cf-4b99-9367-7df45f2073c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = np.arange(9)\n",
    "output.data.add_to_ancil(dict(ids=dtest))\n",
    "output.data.ancil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b87871-e04b-4bb1-9d51-200fe3663b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(output.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae973cd-8e45-4f70-b12b-2f6dbf01570c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b958d33f-c330-4149-aa97-fe1970555bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.evaluation.evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e185eb-397a-41b8-978e-871a9f624cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_eval_dict = dict(\n",
    "    chunk_size=100,\n",
    "    zmin=-1,\n",
    "    zmax=5,\n",
    "    nzbins=200,\n",
    "    epochs=20,\n",
    "    output_name=\"test_evaluator\",\n",
    "    point_metrics=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7417fa3-0c51-4a04-837c-f9e54431eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepEvaluator = Evaluator.make_stage(name=\"DeepDiscEvaluator\", **deep_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ba5ca0-7ded-4b4c-ac63-264c8c23ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res = DeepEvaluator.evaluate(res, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf80908-9582-45ca-b78f-f5163fb415a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f874ba48-786b-46d9-845d-d659c86cdc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qp.metrics.pit import PIT\n",
    "from utils import *  # plot_pit_qq, ks_plot\n",
    "\n",
    "pitobj = PIT(res, ztrue)\n",
    "pit_out_rate = pitobj.evaluate_PIT_outlier_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de28b4-61c2-4d6f-ab20-46a0d369364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qp.ensemble import Ensemble\n",
    "\n",
    "\n",
    "class Sample(Ensemble):\n",
    "    \"\"\"Expand qp.Ensemble to append true redshifts\n",
    "    array, metadata, and specific plots.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, pdfs, zgrid, ztrue, photoz_mode=None, code=\"\", name=\"\", n_quant=100\n",
    "    ):\n",
    "        \"\"\"Class constructor\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pdfs: `ndarray`\n",
    "            photo-z PDFs array, shape=(Ngals, Nbins)\n",
    "        zgrid: `ndarray`\n",
    "            PDF bins centers, shape=(Nbins,)\n",
    "        ztrue: `ndarray`\n",
    "            true redshifts, shape=(Ngals,)\n",
    "        photoz_mode: `ndarray`\n",
    "            photo-z (PDF mode), shape=(Ngals,)\n",
    "        code: `str`, (optional)\n",
    "            algorithm name (for plot legends)\n",
    "        name: `str`, (optional)\n",
    "            sample name (for plot legends)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(interp, data=dict(xvals=zgrid, yvals=pdfs))\n",
    "        self._pdfs = pdfs\n",
    "        self._zgrid = zgrid\n",
    "        self._ztrue = ztrue\n",
    "        self._photoz_mode = photoz_mode\n",
    "        self._code = code\n",
    "        self._name = name\n",
    "        self._n_quant = n_quant\n",
    "        self._pit = None\n",
    "        self._qq = None\n",
    "\n",
    "    @property\n",
    "    def code(self):\n",
    "        \"\"\"Photo-z code/algorithm name\"\"\"\n",
    "        return self._code\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        \"\"\"Sample name\"\"\"\n",
    "        return self._name\n",
    "\n",
    "    @property\n",
    "    def ztrue(self):\n",
    "        \"\"\"True redshifts array\"\"\"\n",
    "        return self._ztrue\n",
    "\n",
    "    @property\n",
    "    def zgrid(self):\n",
    "        \"\"\"Redshift grid (binning)\"\"\"\n",
    "        return self._zgrid\n",
    "\n",
    "    @property\n",
    "    def photoz_mode(self):\n",
    "        \"\"\"Photo-z (mode) array\"\"\"\n",
    "        return self._photoz_mode\n",
    "\n",
    "    @property\n",
    "    def n_quant(self):\n",
    "        return self._n_quant\n",
    "\n",
    "    @property\n",
    "    def pit(self):\n",
    "        if self._pit is None:\n",
    "            pit_array = np.array(\n",
    "                [self[i].cdf(self.ztrue[i])[0][0] for i in range(len(self))]\n",
    "            )\n",
    "            self._pit = pit_array\n",
    "        return self._pit\n",
    "\n",
    "    @property\n",
    "    def qq(self, n_quant=100):\n",
    "        q_theory = np.linspace(0.0, 1.0, n_quant)\n",
    "        q_data = np.quantile(self.pit, q_theory)\n",
    "        self._qq = (q_theory, q_data)\n",
    "        return self._qq\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self._ztrue) != len(self._pdfs):\n",
    "            raise ValueError(\"Number of pdfs and true redshifts do not match!!!\")\n",
    "        return len(self._ztrue)\n",
    "\n",
    "    def __str__(self):\n",
    "        code_str = f\"Algorithm: {self._code}\"\n",
    "        name_str = f\"Sample: {self._name}\"\n",
    "        line_str = \"-\" * (max(len(code_str), len(name_str)))\n",
    "        text = str(\n",
    "            line_str\n",
    "            + \"\\n\"\n",
    "            + name_str\n",
    "            + \"\\n\"\n",
    "            + code_str\n",
    "            + \"\\n\"\n",
    "            + line_str\n",
    "            + \"\\n\"\n",
    "            + f\"{len(self)} PDFs with {len(self.zgrid)} probabilities each \\n\"\n",
    "            + f\"qp representation: {self.gen_class.name} \\n\"\n",
    "            + f\"z grid: {len(self.zgrid)} z values from {np.min(self.zgrid)} to {np.max(self.zgrid)} inclusive\"\n",
    "        )\n",
    "        return text\n",
    "\n",
    "    def plot_pdfs(self, gals, show_ztrue=True, show_photoz_mode=False):\n",
    "        colors = plot_pdfs(\n",
    "            self, gals, show_ztrue=show_ztrue, show_photoz_mode=show_photoz_mode\n",
    "        )\n",
    "        return colors\n",
    "\n",
    "    def plot_old_valid(self, gals=None, colors=None):\n",
    "        old_metrics_table = plot_old_valid(self, gals=gals, colors=colors)\n",
    "        return old_metrics_table\n",
    "\n",
    "    def plot_pit_qq(\n",
    "        self,\n",
    "        bins=None,\n",
    "        label=None,\n",
    "        title=None,\n",
    "        show_pit=True,\n",
    "        show_qq=True,\n",
    "        show_pit_out_rate=True,\n",
    "        savefig=False,\n",
    "    ):\n",
    "        \"\"\"Make plot PIT-QQ as Figure 2 from Schmidt et al. 2020.\"\"\"\n",
    "        fig_filename = plot_pit_qq(\n",
    "            self,\n",
    "            bins=bins,\n",
    "            label=label,\n",
    "            title=title,\n",
    "            show_pit=show_pit,\n",
    "            show_qq=show_qq,\n",
    "            show_pit_out_rate=show_pit_out_rate,\n",
    "            savefig=savefig,\n",
    "        )\n",
    "        return fig_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26258c-8401-45ed-a75d-92da6ed7f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pit_qq(\n",
    "    pdfs,\n",
    "    zgrid,\n",
    "    ztrue,\n",
    "    bins=None,\n",
    "    title=None,\n",
    "    code=None,\n",
    "    show_pit=True,\n",
    "    show_qq=True,\n",
    "    pit_out_rate=None,\n",
    "    savefig=False,\n",
    ") -> str:\n",
    "    \"\"\"Quantile-quantile plot\n",
    "        Ancillary function to be used by class Metrics.\n",
    "    ​\n",
    "        Parameters\n",
    "        ----------\n",
    "        pit: `PIT` object\n",
    "            class from metrics.py\n",
    "        bins: `int`, optional\n",
    "            number of PIT bins\n",
    "            if None, use the same number of quantiles (sample.n_quant)\n",
    "        title: `str`, optional\n",
    "            if None, use formatted sample's name (sample.name)\n",
    "        label: `str`, optional\n",
    "            if None, use formatted code's name (sample.code)\n",
    "        show_pit: `bool`, optional\n",
    "            include PIT histogram (default=True)\n",
    "        show_qq: `bool`, optional\n",
    "            include QQ plot (default=True)\n",
    "        pit_out_rate: `ndarray`, optional\n",
    "            print metric value on the plot panel (default=None)\n",
    "        savefig: `bool`, optional\n",
    "            save plot in .png file (default=False)\n",
    "    \"\"\"\n",
    "\n",
    "    if bins is None:\n",
    "        bins = 100\n",
    "    if title is None:\n",
    "        title = \"\"\n",
    "\n",
    "    if code is None:\n",
    "        code = \"\"\n",
    "        label = \"\"\n",
    "    else:\n",
    "        label = code + \"\\n\"\n",
    "\n",
    "    if pit_out_rate is not None:\n",
    "        try:\n",
    "            label += \"PIT$_{out}$: \"\n",
    "            label += f\"{float(pit_out_rate):.4f}\"\n",
    "        except:\n",
    "            print(\"Unsupported format for pit_out_rate.\")\n",
    "\n",
    "    plt.figure(figsize=[4, 5])\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1])\n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    sample = Sample(pdfs, zgrid, ztrue)\n",
    "\n",
    "    if show_qq:\n",
    "        ax0.plot(\n",
    "            sample.qq[0], sample.qq[1], c=\"r\", linestyle=\"-\", linewidth=3, label=label\n",
    "        )\n",
    "        ax0.plot([0, 1], [0, 1], color=\"k\", linestyle=\"--\", linewidth=2)\n",
    "        ax0.set_ylabel(\"Q$_{data}$\", fontsize=18)\n",
    "        plt.ylim(-0.001, 1.001)\n",
    "    plt.xlim(-0.001, 1.001)\n",
    "    plt.title(title)\n",
    "    if show_pit:\n",
    "        fzdata = Ensemble(interp, data=dict(xvals=zgrid, yvals=pdfs))\n",
    "        pitobj = PIT(fzdata, ztrue)\n",
    "        pit_vals = np.array(pitobj.pit_samps)\n",
    "        pit_out_rate = pitobj.evaluate_PIT_outlier_rate()\n",
    "\n",
    "        try:\n",
    "            y_uni = float(len(pit_vals)) / float(bins)\n",
    "        except:\n",
    "            y_uni = float(len(pit_vals)) / float(len(bins))\n",
    "        if not show_qq:\n",
    "            ax0.hist(pit_vals, bins=bins, alpha=0.7, label=label)\n",
    "            ax0.set_ylabel(\"Number\")\n",
    "            ax0.hlines(y_uni, xmin=0, xmax=1, color=\"k\")\n",
    "            plt.ylim(\n",
    "                0,\n",
    "            )  # -0.001, 1.001)\n",
    "        else:\n",
    "            ax1 = ax0.twinx()\n",
    "            ax1.hist(pit_vals, bins=bins, alpha=0.7)\n",
    "            ax1.set_ylabel(\"Number\")\n",
    "            ax1.hlines(y_uni, xmin=0, xmax=1, color=\"k\")\n",
    "    leg = ax0.legend(handlelength=0, handletextpad=0, fancybox=True)\n",
    "    for item in leg.legendHandles:\n",
    "        item.set_visible(False)\n",
    "    if show_qq:\n",
    "        ax2 = plt.subplot(gs[1])\n",
    "        ax2.plot(\n",
    "            sample.qq[0],\n",
    "            (sample.qq[1] - sample.qq[0]),\n",
    "            c=\"r\",\n",
    "            linestyle=\"-\",\n",
    "            linewidth=3,\n",
    "        )\n",
    "        plt.ylabel(\"$\\Delta$Q\", fontsize=18)\n",
    "        ax2.plot([0, 1], [0, 0], color=\"k\", linestyle=\"--\", linewidth=2)\n",
    "        plt.xlim(-0.001, 1.001)\n",
    "        plt.ylim(\n",
    "            np.min([-0.12, np.min(sample.qq[1] - sample.qq[0]) * 1.05]),\n",
    "            np.max([0.12, np.max(sample.qq[1] - sample.qq[0]) * 1.05]),\n",
    "        )\n",
    "    if show_pit:\n",
    "        if show_qq:\n",
    "            plt.xlabel(\"Q$_{theory}$ / PIT Value\", fontsize=18)\n",
    "        else:\n",
    "            plt.xlabel(\"PIT Value\", fontsize=18)\n",
    "    else:\n",
    "        if show_qq:\n",
    "            plt.xlabel(\"Q$_{theory}$\", fontsize=18)\n",
    "    if savefig:\n",
    "        fig_filename = str(\"plot_pit_qq_\" + f\"{(code).replace(' ', '_')}.png\")\n",
    "        plt.savefig(fig_filename)\n",
    "    else:\n",
    "        fig_filename = None\n",
    "\n",
    "    return fig_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1263c0-1b38-4042-956c-902f1065c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from qp import interp\n",
    "\n",
    "\n",
    "zgrid = np.linspace(-1, 5, 200)\n",
    "pdfs = res.objdata()[\"yvals\"]\n",
    "plot_pit_qq(\n",
    "    pdfs,\n",
    "    zgrid,\n",
    "    ztrue,\n",
    "    title=\"PIT-QQ - toy data\",\n",
    "    code=\"DeepDISC\",\n",
    "    pit_out_rate=pit_out_rate,\n",
    "    savefig=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb4cfe-af95-45f1-91f1-ed9c08009f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.objdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78982146-0b5d-49d3-bb04-77931ea3aa5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b2cf39-8446-4888-b304-85637c2fba92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229bf0e4-d58f-4383-a77d-478ca07f1120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ddrailnv]",
   "language": "python",
   "name": "conda-env-.conda-ddrailnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
