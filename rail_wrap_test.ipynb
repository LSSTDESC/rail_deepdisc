{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c2a2dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drewoldag/.conda/envs/rail_deepdisc39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Training script for LazyConfig models\n",
    "try:\n",
    "    # ignore ShapelyDeprecationWarning from fvcore\n",
    "    import warnings\n",
    "\n",
    "    from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning)\n",
    "\n",
    "except:\n",
    "    pass\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "\n",
    "import detectron2.utils.comm as comm\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2.config import LazyConfig, get_cfg\n",
    "import detectron2.data as data\n",
    "from detectron2.engine import (\n",
    "    launch,\n",
    ")\n",
    "\n",
    "from deepdisc.data_format.augment_image import train_augs\n",
    "from deepdisc.data_format.image_readers import DC2ImageReader\n",
    "from deepdisc.data_format.register_data import (\n",
    "    register_data_set,\n",
    ")  # , register_loaded_data_set\n",
    "from deepdisc.model.loaders import (\n",
    "    RedshiftFlatDictMapper,\n",
    "    return_test_loader,\n",
    "    return_train_loader,\n",
    ")\n",
    "from deepdisc.model.models import (\n",
    "    RedshiftPointCasROIHeads,\n",
    "    RedshiftPointROIHeads,\n",
    "    RedshiftPDFROIHeads,\n",
    "    return_lazy_model,\n",
    ")\n",
    "from deepdisc.training.trainers import (\n",
    "    return_evallosshook,\n",
    "    return_lazy_trainer,\n",
    "    return_optimizer,\n",
    "    return_savehook,\n",
    "    return_schedulerhook,\n",
    ")\n",
    "from deepdisc.utils.parse_arguments import make_training_arg_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6167f21e-0bbc-45cb-bc06-aac14f765fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(rail.estimation.algos.deepdisc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5244b7ff-1501-47ad-b74e-c575f9758089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rail.estimation.algos.deepdisc import DeepDiscInformer\n",
    "import rail\n",
    "from rail.estimation.algos.deepdisc import *\n",
    "from rail.core.data import TableHandle, JsonHandle\n",
    "from rail.core.stage import RailStage\n",
    "\n",
    "from rail.deepdisc.configs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e82e6de-f6ec-46b5-a6a7-60e79cd79c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/home/shared/hsc/DC2/test_data/dataset_3/flattened_images_train_small.npy\n",
    "cfgfile = \"/home/shared/hsc/detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.py\"\n",
    "dirpath = \"./tests/deepdisc/test_data/dc2/\"\n",
    "output_dir = \"./\"\n",
    "output_name = \"test\"\n",
    "\n",
    "#trainfile = dirpath + \"flattened_data_test.npy\"\n",
    "trainfile = \"/home/shared/hsc/DC2/test_data/dataset_3/flattened_images_train.hdf5\"\n",
    "testfile = dirpath + \"flattened_data_test.npy\"\n",
    "metadatafile = \"/home/shared/hsc/DC2/test_data/dataset_3/train_metadata.json\"\n",
    "classes = [\"object\"]\n",
    "numclasses = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ed14575-d55b-4400-90f9-752f27cc75c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'detectron2://ImageNetPretrained/MSRA/R-50.pkl'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = get_lazy_config(cfgfile, 1, 1)\n",
    "cfg.train.init_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffd06982-1e98-4b8a-a3a8-b68ac71cd6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9940c5c2-b692-4e3b-a595-0afec0c21e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepdisc.data_format.file_io import get_data_from_json\n",
    "\n",
    "#testdata = np.load(trainfile, allow_pickle=True)\n",
    "testdata = DS.read_file(\"testdata\", TableHandle, trainfile)\n",
    "metadata = get_data_from_json(metadatafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef5887b5-f2d2-4047-8d81-af04dfa54bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testdata = np.load(\"./tests/deepdisc/test_data/dc2/flattened_data_test.npy\")\n",
    "mapper = RedshiftFlatDictMapper().map_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12521313-e75d-4ebb-afd4-03d65a3d6931",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('images',\n",
       "              array([[-0.12334768, -0.02265165, -0.08154224, ...,  0.11944132,\n",
       "                       0.26857075, -0.44378543],\n",
       "                     [-0.12334768, -0.02265165, -0.08154224, ...,  0.11944132,\n",
       "                       0.26857075, -0.44378543],\n",
       "                     [-0.12334768, -0.02265165, -0.08154224, ...,  0.11944132,\n",
       "                       0.26857075, -0.44378543],\n",
       "                     ...,\n",
       "                     [-0.12334768, -0.02265165, -0.08154224, ...,  0.11944132,\n",
       "                       0.26857075, -0.44378543],\n",
       "                     [-0.12334768, -0.02265165, -0.08154224, ...,  0.11944132,\n",
       "                       0.26857075, -0.44378543],\n",
       "                     [-0.12334768, -0.02265165, -0.08154224, ...,  0.11944132,\n",
       "                       0.26857075, -0.44378543]], dtype=float32))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7e9a2e4-0bae-4b0d-9e62-60fd09795e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dicts = {}\n",
    "# dds = []\n",
    "# for row in testdata:\n",
    "#     dds.append(mapper(row))\n",
    "# dataset_dicts[\"test\"] = dds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01f48f9e-16a3-423d-a550-ed9298b6fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = DS.add_data(\"training\", testdata(), TableHandle, path=trainfile) #()[\"images\"]\n",
    "testing = DS.add_data(\"testing\", testdata, TableHandle)\n",
    "\n",
    "metadatahandle = DS.add_data(\"metadata\", metadata, JsonHandle, path=metadatafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd1e520f-617e-4a5a-91ec-0c98cf037c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['images']>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "f = h5py.File(trainfile, 'r')\n",
    "\n",
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9fcdcff-6bb5-474b-8f91-8904a6211f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rail.core.data.TableHandle at 0x7ff6e3d29460>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4b8aad5-1fcd-4ab1-a843-899c8913e297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'rail.core.data.TableHandle'> /home/shared/hsc/DC2/test_data/dataset_3/flattened_images_train.hdf5, (wd)\n"
     ]
    }
   ],
   "source": [
    "print(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a57e206a-c5c6-4526-a278-868ef2a6047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_dict = dict(\n",
    "    chunk_size=100,\n",
    "    epochs=20,\n",
    "    numclasses=1,\n",
    "    batch_size=1,\n",
    "    output_dir=\"./\",\n",
    "    cfgfile=\"/home/shared/hsc/detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.py\",\n",
    "    output_name=\"test_informer\",\n",
    "#     hdf5_groupname=\"images\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d611ae20-a7c7-4f52-9390-1d27eca138f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inform = DeepDiscInformer.make_stage(name='Inform_DeepDISC', model='detectron2://ImageNetPretrained/MSRA/R-50.pkl', **deep_dict)\n",
    "Inform = DeepDiscInformer.make_stage(\n",
    "    name=\"Inform_DeepDISC\", model=\"test_informer.pkl\", **deep_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87b6a9d1-697e-4bde-abee-e519253daa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[12/06 18:02:53 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[12/06 18:02:53 d2.data.common]: \u001b[0mSerializing 1000 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[12/06 18:03:07 d2.data.common]: \u001b[0mSerialized dataset takes 274.77 MiB\n",
      "\u001b[32m[12/06 18:03:07 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=1\n",
      "\u001b[32m[12/06 18:03:07 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[12/06 18:03:07 d2.data.common]: \u001b[0mSerializing 1000 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[12/06 18:03:23 d2.data.common]: \u001b[0mSerialized dataset takes 274.77 MiB\n",
      "\u001b[32m[12/06 18:03:23 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...\n",
      "\u001b[32m[12/06 18:03:23 d2.checkpoint.c2_model_loading]: \u001b[0mRenaming Caffe2 weights ......\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[12/06 18:03:23 d2.checkpoint.c2_model_loading]: \u001b[0mShape of stem.conv1.weight in checkpoint is torch.Size([64, 3, 7, 7]), while shape of backbone.bottom_up.stem.conv1.weight in model is torch.Size([64, 6, 7, 7]).\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[12/06 18:03:23 d2.checkpoint.c2_model_loading]: \u001b[0mstem.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "\u001b[32m[12/06 18:03:23 d2.checkpoint.c2_model_loading]: \u001b[0mFollowing weights matched with submodule backbone.bottom_up - Total num: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.bottom_up.stem.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral4.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral5.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output4.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output5.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.conv.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.deconv.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn1.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn2.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn3.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn4.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.redshift_fc.0.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.redshift_fc.2.{bias, weight}\u001b[0m\n",
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mfc1000.{bias, weight}\u001b[0m\n",
      "  \u001b[35mstem.conv1.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training:\n",
      "\u001b[32m[12/06 18:03:23 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "<detectron2.solver.lr_scheduler.LRMultiplier object at 0x7ff6c425cdc0>\n",
      "Iteration:  5  time:  2.1094456315040588e-07 dict_keys(['loss_cls', 'loss_box_reg', 'redshift_loss', 'loss_rpn_cls', 'loss_rpn_loc']) [0.5209605097770691, 0.21674169600009918, 0.1313587874174118, 0.5979374051094055, 0.24812164902687073] val loss:  0 lr:  [0.001]\n",
      "Iteration:  10  time:  2.849847078323364e-07 dict_keys(['loss_cls', 'loss_box_reg', 'redshift_loss', 'loss_rpn_cls', 'loss_rpn_loc']) [0.5204330682754517, 0.16198083758354187, 0.10976394265890121, 0.5046432018280029, 0.1870017945766449] val loss:  0 lr:  [0.001]\n",
      "Iteration:  15  time:  2.7194619178771973e-07 dict_keys(['loss_cls', 'loss_box_reg', 'redshift_loss', 'loss_rpn_cls', 'loss_rpn_loc']) [0.5351084470748901, 0.20612120628356934, 0.08991051465272903, 0.3493815064430237, 0.1527405083179474] val loss:  0 lr:  [0.001]\n",
      "Iteration:  20  time:  5.67873939871788e-07 dict_keys(['loss_cls', 'loss_box_reg', 'redshift_loss', 'loss_rpn_cls', 'loss_rpn_loc']) [0.5021840929985046, 0.2904711365699768, 0.09828558564186096, 0.21256855130195618, 0.22599127888679504] val loss:  0 lr:  [0.001]\n",
      "saving test_informer\n",
      "Inserting handle into data store.  model_Inform_DeepDISC: inprogress_test_informer.pkl, Inform_DeepDISC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rail.core.data.ModelHandle at 0x7fff91f43af0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Inform.inform(training, metadatahandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c69b958",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4092931-9c07-4f05-8f72-41cc3430f6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7360caf2-5b3d-4998-be0c-3ca28fb5e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator = DeepDiscEstimator.make_stage(name='DeepDiscEstimator',\n",
    "#                                       model=Inform.get_handle('model'), **deep_dict)\n",
    "\n",
    "Estimator = DeepDiscPDFEstimator.make_stage(\n",
    "    name=\"DeepDiscEstimator\",\n",
    "    model=Inform.get_handle(\"model\"),\n",
    "    hdf5_groupname=\"images\",\n",
    "    **deep_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c74bb222-f53c-415c-acab-f8b978a2af5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mEstimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatahandle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/rail_deepdisc/src/rail/estimation/algos/deepdisc.py:217\u001b[0m, in \u001b[0;36mDeepDiscPDFEstimator.estimate\u001b[0;34m(self, input_data, metadata)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m, input_data)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m, metadata)\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalize()\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_handle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/code/rail_deepdisc/src/rail/estimation/algos/deepdisc.py:236\u001b[0m, in \u001b[0;36mDeepDiscPDFEstimator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    233\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    234\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 236\u001b[0m itr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_idx, _, chunk \u001b[38;5;129;01min\u001b[39;00m itr:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "File \u001b[0;32m~/code/rail_base/src/rail/core/stage.py:338\u001b[0m, in \u001b[0;36mRailStage.input_iterator\u001b[0;34m(self, tag, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhdf5_groupname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;129;01mand\u001b[39;00m handle\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_length \u001b[38;5;241m=\u001b[39m \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroupname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhdf5_groupname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m     total_chunks_needed \u001b[38;5;241m=\u001b[39m ceil(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_length\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mchunk_size)\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;66;03m# If the number of process is larger than we need, we wemove some of them\u001b[39;00m\n",
      "File \u001b[0;32m~/code/rail_base/src/rail/core/data.py:147\u001b[0m, in \u001b[0;36mDataHandle.size\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msize\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    146\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the size of the data associated to this handle\"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/rail_base/src/rail/core/data.py:224\u001b[0m, in \u001b[0;36mTableHandle._size\u001b[0;34m(cls, path, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_size\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtables_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetInputDataLengthHdf5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/rail_deepdisc39/lib/python3.9/site-packages/tables_io/ioUtils.py:86\u001b[0m, in \u001b[0;36mgetInputDataLengthHdf5\u001b[0;34m(filepath, groupname)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Open an HDF5 file and return the size of a group\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mNormally that is what you want to be iterating over.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m hg, infp \u001b[38;5;241m=\u001b[39m readHdf5Group(filepath, groupname)\n\u001b[0;32m---> 86\u001b[0m nrow \u001b[38;5;241m=\u001b[39m \u001b[43mgetGroupInputDataLength\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m infp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nrow\n",
      "File \u001b[0;32m~/.conda/envs/rail_deepdisc39/lib/python3.9/site-packages/tables_io/arrayUtils.py:88\u001b[0m, in \u001b[0;36mgetGroupInputDataLength\u001b[0;34m(hg)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetGroupInputDataLength\u001b[39m(hg):\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the length of a HDF5 group\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    should be the same length\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     firstkey \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mhg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m())[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     89\u001b[0m     nrows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(hg[firstkey])\n\u001b[1;32m     90\u001b[0m     firstname \u001b[38;5;241m=\u001b[39m hg[firstkey]\u001b[38;5;241m.\u001b[39mname\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "results = Estimator.estimate(training, metadatahandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b402ba2-3779-44e7-8654-113f19731464",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "res = results.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42326512-837e-4ac6-87c6-2d59c90312db",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7417fa3-0c51-4a04-837c-f9e54431eeda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rail_deepdisc39",
   "language": "python",
   "name": "rail_deepdisc39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
