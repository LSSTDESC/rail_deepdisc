{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c2a2dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training script for LazyConfig models\n",
    "try:\n",
    "    # ignore ShapelyDeprecationWarning from fvcore\n",
    "    import warnings\n",
    "\n",
    "    from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning)\n",
    "\n",
    "except:\n",
    "    pass\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "\n",
    "import detectron2.utils.comm as comm\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2.config import LazyConfig, get_cfg\n",
    "import detectron2.data as data\n",
    "from detectron2.engine import (\n",
    "    launch,\n",
    ")\n",
    "\n",
    "from deepdisc.data_format.augment_image import train_augs\n",
    "from deepdisc.data_format.image_readers import DC2ImageReader\n",
    "from deepdisc.data_format.register_data import (\n",
    "    register_data_set,\n",
    ")  # , register_loaded_data_set\n",
    "from deepdisc.model.loaders import (\n",
    "    RedshiftFlatDictMapper,\n",
    "    return_test_loader,\n",
    "    return_train_loader,\n",
    ")\n",
    "from deepdisc.model.models import (\n",
    "    RedshiftPointCasROIHeads,\n",
    "    RedshiftPointROIHeads,\n",
    "    RedshiftPDFROIHeads,\n",
    "    return_lazy_model,\n",
    ")\n",
    "from deepdisc.training.trainers import (\n",
    "    return_evallosshook,\n",
    "    return_lazy_trainer,\n",
    "    return_optimizer,\n",
    "    return_savehook,\n",
    "    return_schedulerhook,\n",
    ")\n",
    "from deepdisc.utils.parse_arguments import make_training_arg_parser\n",
    "from deepdisc.inference.predictors import return_predictor_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5244b7ff-1501-47ad-b74e-c575f9758089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rail.estimation.algos.deepdisc import DeepDiscInformer\n",
    "import rail\n",
    "from rail.estimation.algos.deepdisc import *\n",
    "from rail.core.data import TableHandle, JsonHandle\n",
    "from rail.core.stage import RailStage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e82e6de-f6ec-46b5-a6a7-60e79cd79c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfgfile = \"./tests/configs/solo/solo_res50_DC2.py\"\n",
    "cfgfile = \"./configs/solo/solo_swin_DC2_weighted.py\"\n",
    "\n",
    "output_dir = \"./\"\n",
    "output_name = \"test\"\n",
    "\n",
    "trainfile = \"/home/shared/hsc/DC2/test_data/dataset_3/flattened_images_train.hdf5\"\n",
    "testfile = \"/home/shared/hsc/DC2/test_data/dataset_3/flattened_images_test.hdf5\"\n",
    "metadatafile = \"/home/shared/hsc/DC2/test_data/dataset_3/train_metadata.hdf5\"\n",
    "test_metadatafile = \"/home/shared/hsc/DC2/test_data/dataset_3/test_metadata.hdf5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd06982-1e98-4b8a-a3a8-b68ac71cd6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01f48f9e-16a3-423d-a550-ed9298b6fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = DS.add_data(\n",
    "    \"training\", data=None, handle_class=TableHandle, path=trainfile\n",
    ")  # ()[\"images\"]\n",
    "testing = DS.add_data(\"testing\", data=None, handle_class=TableHandle, path=testfile)\n",
    "\n",
    "#metadatahandle = DS.add_data(\"metadata\", metadata, JsonHandle, path=metadatafile)\n",
    "metadatahandle = DS.add_data(\"metadata\", data=None, handle_class=Hdf5Handle, path=metadatafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a57e206a-c5c6-4526-a278-868ef2a6047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_dict = dict(\n",
    "    chunk_size=5,\n",
    "    epoch=20,\n",
    "    batch_size=1,\n",
    "    output_dir=\"./\",\n",
    "    cfgfile = cfgfile,\n",
    "    num_gpus=1,\n",
    "    print_frequency=10,\n",
    "    head_epochs=3,\n",
    "    full_epochs=3,\n",
    "    mile1=0,\n",
    "    mile2=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d611ae20-a7c7-4f52-9390-1d27eca138f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Inform = DeepDiscInformer.make_stage(\n",
    "    name=\"Inform_DeepDISC\", model=\"test_informer.pkl\", **deep_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87b6a9d1-697e-4bde-abee-e519253daa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching data\n",
      "Training head layers\n",
      "\u001b[32m[01/11 14:52:32 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[01/11 14:52:32 d2.data.common]: \u001b[0mSerializing 800 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[01/11 14:52:38 d2.data.common]: \u001b[0mSerialized dataset takes 223.74 MiB\n",
      "\u001b[32m[01/11 14:52:38 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=1\n",
      "\u001b[32m[01/11 14:52:38 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[01/11 14:52:38 d2.data.common]: \u001b[0mSerializing 200 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[01/11 14:52:42 d2.data.common]: \u001b[0mSerialized dataset takes 55.56 MiB\n",
      "\u001b[32m[01/11 14:53:04 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /home/shared/hsc/detectron2/projects/ViTDet/model_final_246a82.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'backbone.bottom_up.patch_embed.proj.weight' to the model due to incompatible shapes: (128, 3, 4, 4) in the checkpoint but (128, 6, 4, 4) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.0.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.0.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.1.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.1.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.2.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.2.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.bottom_up.patch_embed.proj.weight\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.0.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.1.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.2.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.dummy_param\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.redshift_fc.0.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.redshift_fc.2.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.redshift_fc.4.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/11 14:53:06 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "<detectron2.solver.lr_scheduler.LRMultiplier object at 0x7ffcfc273ac0>\n",
      "Iteration:  10  data time:  0.1610238580033183  loss time:  0.21898144111037254 dict_keys(['loss_cls_stage0', 'loss_box_reg_stage0', 'loss_cls_stage1', 'loss_box_reg_stage1', 'loss_cls_stage2', 'loss_box_reg_stage2', 'loss_mask', 'redshift_loss', 'loss_rpn_cls', 'loss_rpn_loc']) [0.6570164561271667, 0.253423810005188, 0.7906529903411865, 0.5083292722702026, 0.8201364278793335, 0.6984100937843323, 0.5414761900901794, 0.6329092184527327, 0.6179696321487427, 0.2329884171485901] val loss:  0 lr:  [0.001]\n",
      "Iteration:  20  data time:  0.2665300415828824  loss time:  0.28549076057970524 dict_keys(['loss_cls_stage0', 'loss_box_reg_stage0', 'loss_cls_stage1', 'loss_box_reg_stage1', 'loss_cls_stage2', 'loss_box_reg_stage2', 'loss_mask', 'redshift_loss', 'loss_rpn_cls', 'loss_rpn_loc']) [0.48118528723716736, 0.2181326299905777, 0.4371892511844635, 0.44899851083755493, 0.47524160146713257, 0.8531872034072876, 0.39002716541290283, 0.566153337333889, 0.18766610324382782, 0.18657097220420837] val loss:  0 lr:  [0.001]\n",
      "Iteration:  30  data time:  0.23754397965967655  loss time:  0.26951733604073524 dict_keys(['loss_cls_stage0', 'loss_box_reg_stage0', 'loss_cls_stage1', 'loss_box_reg_stage1', 'loss_cls_stage2', 'loss_box_reg_stage2', 'loss_mask', 'redshift_loss', 'loss_rpn_cls', 'loss_rpn_loc']) [0.46790915727615356, 0.1698581874370575, 0.43408679962158203, 0.1681375503540039, 0.4060381054878235, 0.22013822197914124, 0.3246963322162628, 0.5708768610414903, 0.16530369222164154, 0.17707106471061707] val loss:  4.067445668278673 lr:  [0.001]\n",
      "Iteration:  40  data time:  0.2505807662382722  loss time:  0.3129308456555009 dict_keys(['loss_cls_stage0', 'loss_box_reg_stage0', 'loss_cls_stage1', 'loss_box_reg_stage1', 'loss_cls_stage2', 'loss_box_reg_stage2', 'loss_mask', 'redshift_loss', 'loss_rpn_cls', 'loss_rpn_loc']) [0.5137966275215149, 0.23343662917613983, 0.4842168092727661, 0.3746652901172638, 0.4666685461997986, 0.5732879638671875, 0.35995176434516907, 0.6052192800435198, 0.0869104340672493, 0.1635773628950119] val loss:  4.067445668278673 lr:  [0.001]\n",
      "Iteration:  50  data time:  0.25889869779348373  loss time:  0.2923493329435587 dict_keys(['loss_cls_stage0', 'loss_box_reg_stage0', 'loss_cls_stage1', 'loss_box_reg_stage1', 'loss_cls_stage2', 'loss_box_reg_stage2', 'loss_mask', 'redshift_loss', 'loss_rpn_cls', 'loss_rpn_loc']) [0.5130445957183838, 0.2899795174598694, 0.4762327969074249, 0.44990047812461853, 0.4576851427555084, 0.6941797733306885, 0.3485666811466217, 0.5322238402194621, 0.1309332400560379, 0.15978659689426422] val loss:  3.8139204196330425 lr:  [0.001]\n",
      "Iteration:  60  data time:  0.2526147188618779  loss time:  0.29025427903980017 dict_keys(['loss_cls_stage0', 'loss_box_reg_stage0', 'loss_cls_stage1', 'loss_box_reg_stage1', 'loss_cls_stage2', 'loss_box_reg_stage2', 'loss_mask', 'redshift_loss', 'loss_rpn_cls', 'loss_rpn_loc']) [0.533742368221283, 0.23051229119300842, 0.47432899475097656, 0.4036557972431183, 0.4625312387943268, 0.6888152360916138, 0.33035022020339966, 0.563372930109306, 0.11668124049901962, 0.14822867512702942] val loss:  3.8139204196330425 lr:  [0.001]\n",
      "saving run\n",
      "Training full model\n",
      "\u001b[32m[01/11 14:59:33 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[01/11 14:59:33 d2.data.common]: \u001b[0mSerializing 800 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[01/11 14:59:36 d2.data.common]: \u001b[0mSerialized dataset takes 223.74 MiB\n",
      "\u001b[32m[01/11 14:59:36 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=1\n",
      "\u001b[32m[01/11 14:59:36 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[01/11 14:59:36 d2.data.common]: \u001b[0mSerializing 200 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[01/11 14:59:37 d2.data.common]: \u001b[0mSerialized dataset takes 55.56 MiB\n",
      "\u001b[32m[01/11 14:59:46 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./run.pth ...\n",
      "\u001b[32m[01/11 14:59:47 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "<detectron2.solver.lr_scheduler.LRMultiplier object at 0x7ffcf403b100>\n",
      "Iteration:  10  data time:  0.2549617039039731  loss time:  0.29319911543279886 dict_keys(['loss_cls_stage0', 'loss_box_reg_stage0', 'loss_cls_stage1', 'loss_box_reg_stage1', 'loss_cls_stage2', 'loss_box_reg_stage2', 'loss_mask', 'redshift_loss', 'loss_rpn_cls', 'loss_rpn_loc']) [0.5522539615631104, 0.2871376872062683, 0.45555785298347473, 0.32853007316589355, 0.4469658434391022, 0.3471643924713135, 0.3571380376815796, 0.5906889465561521, 0.09220621734857559, 0.15008416771888733] val loss:  0 lr:  [0.001, 0.001]\n",
      "Iteration:  20  data time:  0.2577147772535682  loss time:  0.30686087906360626 dict_keys(['loss_cls_stage0', 'loss_box_reg_stage0', 'loss_cls_stage1', 'loss_box_reg_stage1', 'loss_cls_stage2', 'loss_box_reg_stage2', 'loss_mask', 'redshift_loss', 'loss_rpn_cls', 'loss_rpn_loc']) [0.47531065344810486, 0.1835775375366211, 0.42659521102905273, 0.2745348811149597, 0.41826602816581726, 0.45982056856155396, 0.34996646642684937, 0.5825073541108177, 0.03435715287923813, 0.14715588092803955] val loss:  0 lr:  [0.001, 0.001]\n",
      "Iteration:  30  data time:  0.28869335912168026  loss time:  0.304493167437613 dict_keys(['loss_cls_stage0', 'loss_box_reg_stage0', 'loss_cls_stage1', 'loss_box_reg_stage1', 'loss_cls_stage2', 'loss_box_reg_stage2', 'loss_mask', 'redshift_loss', 'loss_rpn_cls', 'loss_rpn_loc']) [0.5466227531433105, 0.1982266902923584, 0.49360013008117676, 0.3038342297077179, 0.4860314428806305, 0.4064161777496338, 0.32074591517448425, 0.5374326575302553, 0.09169284254312515, 0.17126941680908203] val loss:  3.5803670631808697 lr:  [0.001, 0.001]\n",
      "Iteration:  40  data time:  0.32121701445430517  loss time:  0.33528718911111355 dict_keys(['loss_cls_stage0', 'loss_box_reg_stage0', 'loss_cls_stage1', 'loss_box_reg_stage1', 'loss_cls_stage2', 'loss_box_reg_stage2', 'loss_mask', 'redshift_loss', 'loss_rpn_cls', 'loss_rpn_loc']) [0.5667564868927002, 0.2239982932806015, 0.466813862323761, 0.3266463875770569, 0.4065093696117401, 0.3632262945175171, 0.3428439199924469, 0.5206037626010044, 0.057056400924921036, 0.17303960025310516] val loss:  3.5803670631808697 lr:  [0.001, 0.001]\n",
      "Iteration:  50  data time:  0.2906856117770076  loss time:  0.31192955654114485 dict_keys(['loss_cls_stage0', 'loss_box_reg_stage0', 'loss_cls_stage1', 'loss_box_reg_stage1', 'loss_cls_stage2', 'loss_box_reg_stage2', 'loss_mask', 'redshift_loss', 'loss_rpn_cls', 'loss_rpn_loc']) [0.5712732076644897, 0.21828371286392212, 0.46136540174484253, 0.26984184980392456, 0.47813498973846436, 0.27140772342681885, 0.32365795969963074, 0.5320942684168346, 0.06894038617610931, 0.1562022715806961] val loss:  3.56992296614268 lr:  [0.001, 0.001]\n",
      "Iteration:  60  data time:  0.2757276641204953  loss time:  0.2912742532789707 dict_keys(['loss_cls_stage0', 'loss_box_reg_stage0', 'loss_cls_stage1', 'loss_box_reg_stage1', 'loss_cls_stage2', 'loss_box_reg_stage2', 'loss_mask', 'redshift_loss', 'loss_rpn_cls', 'loss_rpn_loc']) [0.47808071970939636, 0.2537722587585449, 0.46167922019958496, 0.41390788555145264, 0.42677757143974304, 0.4897868037223816, 0.32311010360717773, 0.5776587087133268, 0.06243099272251129, 0.13866765797138214] val loss:  3.56992296614268 lr:  [0.001, 0.001]\n",
      "saving run\n",
      "Inserting handle into data store.  model_Inform_DeepDISC: inprogress_test_informer.pkl, Inform_DeepDISC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rail.core.data.ModelHandle at 0x7ffcfc14c910>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inform.inform(training, metadatahandle)\n",
    "Inform.inform(training, metadatahandle) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c69b958",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7070dcb2-0de8-42af-af8d-f40c689b7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadatahandle = DS.add_data(\n",
    "#     \"metadata\", test_metadata, JsonHandle, path=test_metadatafile\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7360caf2-5b3d-4998-be0c-3ca28fb5e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator = DeepDiscPDFEstimator.make_stage(\n",
    "#     name=\"DeepDiscEstimator\",\n",
    "#     model=Inform.get_handle(\"model\"),\n",
    "#     # hdf5_groupname=\"images\",\n",
    "#     **deep_dict,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415a174-6d14-48f5-9241-963a84431501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estimator.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74bb222-f53c-415c-acab-f8b978a2af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = Estimator.estimate(testing, metadatahandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b402ba2-3779-44e7-8654-113f19731464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = results.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2b4a5-0422-4f8f-8e08-49811549c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truth = Estimator.get_handle(\"truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da4c4a-93c6-4203-a1b3-4fe186bad242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ztrue = truth.data[\"redshift\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48290de-6f18-422c-9b04-c6aaec97439f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c712fc8-1e60-4389-86fb-2864922fe19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a243e22-46b1-47a2-9c31-9e0746dbee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from rail.core.data import ModelHandle\n",
    "MH = ModelHandle('model',path='./test_informer.pkl')\n",
    "data = MH.read()\n",
    "#cfgfile = \"./configs/solo/solo_swin_DC2.py\"\n",
    "#cfg = LazyConfig.load(cfgfile)\n",
    "#checkpoint = MH()['nnmodel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15fb9820-e604-4fff-95ba-c549311ebd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d93d6cc-333c-4c28-b5df-f295433cc9e7",
   "metadata": {},
   "source": [
    "## Test new chunking algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48936320-ed0f-40c3-8329-1f3e2c7c31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_estimation_dict = dict(\n",
    "    chunk_size=5,\n",
    "    output_dir=\"./\",\n",
    "    cfgfile = cfgfile,\n",
    "    zmin=0,\n",
    "    zmax=5,\n",
    "    nzbins=200,\n",
    "    output_mode='default',\n",
    "    include_ids=True\n",
    ")\n",
    "\n",
    "EstimatorWithChunks = DeepDiscPDFEstimatorWithChunking.make_stage(\n",
    "    name=\"DeepDiscEstimatorWithChunks\",\n",
    "    #model=Inform.get_handle(\"model\"),\n",
    "    model=MH,\n",
    "    **deep_estimation_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31f8a0c3-6ad4-430c-9892-b70ae4285ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_for_chunks = \"/home/shared/hsc/DC2/test_data/dataset_3/flattened_images_test_small.hdf5\"\n",
    "test_handle_for_chunks = DS.add_data(\"testing\", data=None, handle_class=TableHandle, path=test_file_for_chunks)\n",
    "metadatafile_with_chunks = \"/home/shared/hsc/DC2/test_data/dataset_3/test_metadata_example.hdf5\"\n",
    "metadatahandle_with_chunks = DS.add_data(\"metadata\", data=None, handle_class=Hdf5Handle, path=metadatafile_with_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c684e258-6c71-476f-bf35-79b53df86282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting handle into data store.  model: ./test_informer.pkl, DeepDiscEstimatorWithChunks\n",
      "Caching data\n",
      "Processing chunk (start:end) - (0:5)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Processing chunk (start:end) - (5:10)\n",
      "Matching objects\n",
      "Adding PDFs to ensemble\n",
      "Adding true Z to ensemble\n",
      "Adding object IDs to ensemble\n",
      "Writing out this temporary ensemble to disk\n",
      "Inserting handle into data store.  output_DeepDiscEstimatorWithChunks: inprogress_output_DeepDiscEstimatorWithChunks.hdf5, DeepDiscEstimatorWithChunks\n"
     ]
    }
   ],
   "source": [
    "results_from_chunks = EstimatorWithChunks.estimate(test_handle_for_chunks, metadatahandle_with_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e8f2a02-2aec-46cb-b963-c32b3caa0711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ens = results_from_chunks.read()\n",
    "res_ens.npdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57dee5cd-d2ad-4132-83a9-3845aeb8bbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': array([[0.60301508],\n",
       "        [0.65326633],\n",
       "        [0.72864322],\n",
       "        [0.60301508],\n",
       "        [0.67839196],\n",
       "        [0.60301508],\n",
       "        [0.45226131],\n",
       "        [0.67839196],\n",
       "        [0.65326633],\n",
       "        [0.72864322],\n",
       "        [0.60301508]]),\n",
       " 'true_zs': array([0.76212734, 1.9214625 , 0.72423804, 1.7151054 , 1.9245776 ,\n",
       "        0.9738993 , 1.4810951 , 2.9458368 , 1.937903  , 1.7834353 ,\n",
       "        2.5230374 ]),\n",
       " 'ids': array([16836422124214204, 16836422124182002, 16836422124215466,\n",
       "        16836422124181685, 16836422124188454, 16835863778463200,\n",
       "        16836014102285230, 16836014102286361, 16840412148798758,\n",
       "        16840412148831225, 16840412148798452])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ens.ancil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac614133-deb4-4fd6-be40-ed5750ff13e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = EstimatorWithChunks.get_handle(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1513a0-2297-4c6c-807a-83faa995bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.data.ancil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071207a1-b3cf-4b99-9367-7df45f2073c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = np.arange(9)\n",
    "output.data.add_to_ancil(dict(ids=dtest))\n",
    "output.data.ancil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b87871-e04b-4bb1-9d51-200fe3663b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(output.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae973cd-8e45-4f70-b12b-2f6dbf01570c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b958d33f-c330-4149-aa97-fe1970555bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.evaluation.evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e185eb-397a-41b8-978e-871a9f624cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_eval_dict = dict(\n",
    "    chunk_size=100,\n",
    "    zmin=-1,\n",
    "    zmax=5,\n",
    "    nzbins=200,\n",
    "    epochs=20,\n",
    "    output_name=\"test_evaluator\",\n",
    "    point_metrics=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7417fa3-0c51-4a04-837c-f9e54431eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepEvaluator = Evaluator.make_stage(name=\"DeepDiscEvaluator\", **deep_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ba5ca0-7ded-4b4c-ac63-264c8c23ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res = DeepEvaluator.evaluate(res, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf80908-9582-45ca-b78f-f5163fb415a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f874ba48-786b-46d9-845d-d659c86cdc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qp.metrics.pit import PIT\n",
    "from utils import *  # plot_pit_qq, ks_plot\n",
    "\n",
    "pitobj = PIT(res, ztrue)\n",
    "pit_out_rate = pitobj.evaluate_PIT_outlier_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de28b4-61c2-4d6f-ab20-46a0d369364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qp.ensemble import Ensemble\n",
    "\n",
    "\n",
    "class Sample(Ensemble):\n",
    "    \"\"\"Expand qp.Ensemble to append true redshifts\n",
    "    array, metadata, and specific plots.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, pdfs, zgrid, ztrue, photoz_mode=None, code=\"\", name=\"\", n_quant=100\n",
    "    ):\n",
    "        \"\"\"Class constructor\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pdfs: `ndarray`\n",
    "            photo-z PDFs array, shape=(Ngals, Nbins)\n",
    "        zgrid: `ndarray`\n",
    "            PDF bins centers, shape=(Nbins,)\n",
    "        ztrue: `ndarray`\n",
    "            true redshifts, shape=(Ngals,)\n",
    "        photoz_mode: `ndarray`\n",
    "            photo-z (PDF mode), shape=(Ngals,)\n",
    "        code: `str`, (optional)\n",
    "            algorithm name (for plot legends)\n",
    "        name: `str`, (optional)\n",
    "            sample name (for plot legends)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(interp, data=dict(xvals=zgrid, yvals=pdfs))\n",
    "        self._pdfs = pdfs\n",
    "        self._zgrid = zgrid\n",
    "        self._ztrue = ztrue\n",
    "        self._photoz_mode = photoz_mode\n",
    "        self._code = code\n",
    "        self._name = name\n",
    "        self._n_quant = n_quant\n",
    "        self._pit = None\n",
    "        self._qq = None\n",
    "\n",
    "    @property\n",
    "    def code(self):\n",
    "        \"\"\"Photo-z code/algorithm name\"\"\"\n",
    "        return self._code\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        \"\"\"Sample name\"\"\"\n",
    "        return self._name\n",
    "\n",
    "    @property\n",
    "    def ztrue(self):\n",
    "        \"\"\"True redshifts array\"\"\"\n",
    "        return self._ztrue\n",
    "\n",
    "    @property\n",
    "    def zgrid(self):\n",
    "        \"\"\"Redshift grid (binning)\"\"\"\n",
    "        return self._zgrid\n",
    "\n",
    "    @property\n",
    "    def photoz_mode(self):\n",
    "        \"\"\"Photo-z (mode) array\"\"\"\n",
    "        return self._photoz_mode\n",
    "\n",
    "    @property\n",
    "    def n_quant(self):\n",
    "        return self._n_quant\n",
    "\n",
    "    @property\n",
    "    def pit(self):\n",
    "        if self._pit is None:\n",
    "            pit_array = np.array(\n",
    "                [self[i].cdf(self.ztrue[i])[0][0] for i in range(len(self))]\n",
    "            )\n",
    "            self._pit = pit_array\n",
    "        return self._pit\n",
    "\n",
    "    @property\n",
    "    def qq(self, n_quant=100):\n",
    "        q_theory = np.linspace(0.0, 1.0, n_quant)\n",
    "        q_data = np.quantile(self.pit, q_theory)\n",
    "        self._qq = (q_theory, q_data)\n",
    "        return self._qq\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self._ztrue) != len(self._pdfs):\n",
    "            raise ValueError(\"Number of pdfs and true redshifts do not match!!!\")\n",
    "        return len(self._ztrue)\n",
    "\n",
    "    def __str__(self):\n",
    "        code_str = f\"Algorithm: {self._code}\"\n",
    "        name_str = f\"Sample: {self._name}\"\n",
    "        line_str = \"-\" * (max(len(code_str), len(name_str)))\n",
    "        text = str(\n",
    "            line_str\n",
    "            + \"\\n\"\n",
    "            + name_str\n",
    "            + \"\\n\"\n",
    "            + code_str\n",
    "            + \"\\n\"\n",
    "            + line_str\n",
    "            + \"\\n\"\n",
    "            + f\"{len(self)} PDFs with {len(self.zgrid)} probabilities each \\n\"\n",
    "            + f\"qp representation: {self.gen_class.name} \\n\"\n",
    "            + f\"z grid: {len(self.zgrid)} z values from {np.min(self.zgrid)} to {np.max(self.zgrid)} inclusive\"\n",
    "        )\n",
    "        return text\n",
    "\n",
    "    def plot_pdfs(self, gals, show_ztrue=True, show_photoz_mode=False):\n",
    "        colors = plot_pdfs(\n",
    "            self, gals, show_ztrue=show_ztrue, show_photoz_mode=show_photoz_mode\n",
    "        )\n",
    "        return colors\n",
    "\n",
    "    def plot_old_valid(self, gals=None, colors=None):\n",
    "        old_metrics_table = plot_old_valid(self, gals=gals, colors=colors)\n",
    "        return old_metrics_table\n",
    "\n",
    "    def plot_pit_qq(\n",
    "        self,\n",
    "        bins=None,\n",
    "        label=None,\n",
    "        title=None,\n",
    "        show_pit=True,\n",
    "        show_qq=True,\n",
    "        show_pit_out_rate=True,\n",
    "        savefig=False,\n",
    "    ):\n",
    "        \"\"\"Make plot PIT-QQ as Figure 2 from Schmidt et al. 2020.\"\"\"\n",
    "        fig_filename = plot_pit_qq(\n",
    "            self,\n",
    "            bins=bins,\n",
    "            label=label,\n",
    "            title=title,\n",
    "            show_pit=show_pit,\n",
    "            show_qq=show_qq,\n",
    "            show_pit_out_rate=show_pit_out_rate,\n",
    "            savefig=savefig,\n",
    "        )\n",
    "        return fig_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26258c-8401-45ed-a75d-92da6ed7f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pit_qq(\n",
    "    pdfs,\n",
    "    zgrid,\n",
    "    ztrue,\n",
    "    bins=None,\n",
    "    title=None,\n",
    "    code=None,\n",
    "    show_pit=True,\n",
    "    show_qq=True,\n",
    "    pit_out_rate=None,\n",
    "    savefig=False,\n",
    ") -> str:\n",
    "    \"\"\"Quantile-quantile plot\n",
    "        Ancillary function to be used by class Metrics.\n",
    "    ​\n",
    "        Parameters\n",
    "        ----------\n",
    "        pit: `PIT` object\n",
    "            class from metrics.py\n",
    "        bins: `int`, optional\n",
    "            number of PIT bins\n",
    "            if None, use the same number of quantiles (sample.n_quant)\n",
    "        title: `str`, optional\n",
    "            if None, use formatted sample's name (sample.name)\n",
    "        label: `str`, optional\n",
    "            if None, use formatted code's name (sample.code)\n",
    "        show_pit: `bool`, optional\n",
    "            include PIT histogram (default=True)\n",
    "        show_qq: `bool`, optional\n",
    "            include QQ plot (default=True)\n",
    "        pit_out_rate: `ndarray`, optional\n",
    "            print metric value on the plot panel (default=None)\n",
    "        savefig: `bool`, optional\n",
    "            save plot in .png file (default=False)\n",
    "    \"\"\"\n",
    "\n",
    "    if bins is None:\n",
    "        bins = 100\n",
    "    if title is None:\n",
    "        title = \"\"\n",
    "\n",
    "    if code is None:\n",
    "        code = \"\"\n",
    "        label = \"\"\n",
    "    else:\n",
    "        label = code + \"\\n\"\n",
    "\n",
    "    if pit_out_rate is not None:\n",
    "        try:\n",
    "            label += \"PIT$_{out}$: \"\n",
    "            label += f\"{float(pit_out_rate):.4f}\"\n",
    "        except:\n",
    "            print(\"Unsupported format for pit_out_rate.\")\n",
    "\n",
    "    plt.figure(figsize=[4, 5])\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1])\n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    sample = Sample(pdfs, zgrid, ztrue)\n",
    "\n",
    "    if show_qq:\n",
    "        ax0.plot(\n",
    "            sample.qq[0], sample.qq[1], c=\"r\", linestyle=\"-\", linewidth=3, label=label\n",
    "        )\n",
    "        ax0.plot([0, 1], [0, 1], color=\"k\", linestyle=\"--\", linewidth=2)\n",
    "        ax0.set_ylabel(\"Q$_{data}$\", fontsize=18)\n",
    "        plt.ylim(-0.001, 1.001)\n",
    "    plt.xlim(-0.001, 1.001)\n",
    "    plt.title(title)\n",
    "    if show_pit:\n",
    "        fzdata = Ensemble(interp, data=dict(xvals=zgrid, yvals=pdfs))\n",
    "        pitobj = PIT(fzdata, ztrue)\n",
    "        pit_vals = np.array(pitobj.pit_samps)\n",
    "        pit_out_rate = pitobj.evaluate_PIT_outlier_rate()\n",
    "\n",
    "        try:\n",
    "            y_uni = float(len(pit_vals)) / float(bins)\n",
    "        except:\n",
    "            y_uni = float(len(pit_vals)) / float(len(bins))\n",
    "        if not show_qq:\n",
    "            ax0.hist(pit_vals, bins=bins, alpha=0.7, label=label)\n",
    "            ax0.set_ylabel(\"Number\")\n",
    "            ax0.hlines(y_uni, xmin=0, xmax=1, color=\"k\")\n",
    "            plt.ylim(\n",
    "                0,\n",
    "            )  # -0.001, 1.001)\n",
    "        else:\n",
    "            ax1 = ax0.twinx()\n",
    "            ax1.hist(pit_vals, bins=bins, alpha=0.7)\n",
    "            ax1.set_ylabel(\"Number\")\n",
    "            ax1.hlines(y_uni, xmin=0, xmax=1, color=\"k\")\n",
    "    leg = ax0.legend(handlelength=0, handletextpad=0, fancybox=True)\n",
    "    for item in leg.legendHandles:\n",
    "        item.set_visible(False)\n",
    "    if show_qq:\n",
    "        ax2 = plt.subplot(gs[1])\n",
    "        ax2.plot(\n",
    "            sample.qq[0],\n",
    "            (sample.qq[1] - sample.qq[0]),\n",
    "            c=\"r\",\n",
    "            linestyle=\"-\",\n",
    "            linewidth=3,\n",
    "        )\n",
    "        plt.ylabel(\"$\\Delta$Q\", fontsize=18)\n",
    "        ax2.plot([0, 1], [0, 0], color=\"k\", linestyle=\"--\", linewidth=2)\n",
    "        plt.xlim(-0.001, 1.001)\n",
    "        plt.ylim(\n",
    "            np.min([-0.12, np.min(sample.qq[1] - sample.qq[0]) * 1.05]),\n",
    "            np.max([0.12, np.max(sample.qq[1] - sample.qq[0]) * 1.05]),\n",
    "        )\n",
    "    if show_pit:\n",
    "        if show_qq:\n",
    "            plt.xlabel(\"Q$_{theory}$ / PIT Value\", fontsize=18)\n",
    "        else:\n",
    "            plt.xlabel(\"PIT Value\", fontsize=18)\n",
    "    else:\n",
    "        if show_qq:\n",
    "            plt.xlabel(\"Q$_{theory}$\", fontsize=18)\n",
    "    if savefig:\n",
    "        fig_filename = str(\"plot_pit_qq_\" + f\"{(code).replace(' ', '_')}.png\")\n",
    "        plt.savefig(fig_filename)\n",
    "    else:\n",
    "        fig_filename = None\n",
    "\n",
    "    return fig_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1263c0-1b38-4042-956c-902f1065c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from qp import interp\n",
    "\n",
    "\n",
    "zgrid = np.linspace(-1, 5, 200)\n",
    "pdfs = res.objdata()[\"yvals\"]\n",
    "plot_pit_qq(\n",
    "    pdfs,\n",
    "    zgrid,\n",
    "    ztrue,\n",
    "    title=\"PIT-QQ - toy data\",\n",
    "    code=\"DeepDISC\",\n",
    "    pit_out_rate=pit_out_rate,\n",
    "    savefig=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb4cfe-af95-45f1-91f1-ed9c08009f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.objdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78982146-0b5d-49d3-bb04-77931ea3aa5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b2cf39-8446-4888-b304-85637c2fba92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229bf0e4-d58f-4383-a77d-478ca07f1120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ddrailnv]",
   "language": "python",
   "name": "conda-env-.conda-ddrailnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
